{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Success criteria:\n· Must have distinct classes for fully connected and activation layers\n· Must be trainable to solve simple problems i.e XOR gates\n· Must have the capability to show users % error each training epoch",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Analysis:\nThe system will use forward and backward propagation to adjust weights and biases of the overall network. These functions will be implemented by using the Numpy library to access the operations necessary to do so (dot product and matrix transposition). The network will be limited in its scope ude to only containing FC and activation layers, lacking convolutional layers makes the networks unsuitable for image processing, as data would have to be highly altered in order to be processed by the network",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Design:\nAs above, the network will need FC and activation layers, requiring the implementation of forward and back propagation algorithms. Code produced will be primarily object oriented to allow multiple layers to be managed easily when constructing the network overall to solve basic problems. To make the code easier to maintain files will.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The code:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Layer:\n    def __init__(self):\n        self.inp = None\n        self.output = None\n\n    # computes the output given input X\n    def forward_prop(self, input):\n        pass\n\n    # computes dE/dX given dE/dY \n    def backward_prop(self, output_error, learningr):\n        pass",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "First is the basic layer class that other layers will all inherit from, it has empty input/output variables and forward/backward propagation functions, which will be altered depending on the type of layer they are used in.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from layer import Layer\n\n# inherit from basic class Layer\nclass ActivationLayer(Layer):\n    def __init__(self, activation, activation_prime):\n        self.activation = activation\n        self.activation_prime = activation_prime\n\n    # passes input through activation function\n    def forward_prop(self, input_data):\n        self.inp = input_data\n        self.output = self.activation(self.inp)\n        return self.output\n\n    # Returns dE/dX for a known dE/dY.\n    def backward_prop(self, output_error, learningr):\n        return self.activation_prime(self.inp) * output_error",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The second class made is the activation layer, which inherits from the basic layer class. it is initiallised with an activation function and its derivative. for this code Tanh is the activation function used however any sigmoid function can be used instead. The backwards propagation function is simple here as dE/dX = dE/dY * f'(x)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from layer import Layer\nimport numpy as np\n\n# inherit from base class Layer\nclass FCLayer(Layer):\n    def __init__(self, input_size, output_size):\n        self.weights = np.random.rand(input_size, output_size) - 0.5\n        self.bias = np.random.rand(1, output_size) - 0.5\n\n    def forward_prop(self, input_data):\n        self.inp = input_data\n        self.output = np.dot(self.inp, self.weights) + self.bias\n        return self.output\n\n    def backward_prop(self, output_error, learnr):\n        input_error = np.dot(output_error, self.weights.T)\n        weights_error = np.dot(self.inp.T, output_error)\n        \n\n        # update parameters\n        self.weights -= learnr * weights_error\n        self.bias -= learnr * output_error\n        return input_error",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Third class defines the fully connected layer and propogation methods, computing Input.weight + bias = output, and calculating input error using the transposed matrix of weights dotted with the output error. Weight error is calculated by dotting transposed inputs and the output error. Variables are also updated in the backwards prop function (weight and bias).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# loss function and its derivative\ndef mse(y_true, y_pred):\n    return np.mean(np.power(y_true-y_pred, 2))\n\ndef mse_prime(y_true, y_pred):\n    return 2*(y_pred-y_true)/y_true.size",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "functions to calculate error using mean squared error method, though this could be substitued for another method in the network class",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Network:\n    def __init__(self):\n        self.layers = []\n        self.loss = None\n        self.loss_prime = None\n\n    # add layer \n    def add(self, layer):\n        self.layers.append(layer)\n\n    # loss select\n    def use(self, loss, loss_prime):\n        self.loss = loss\n        self.loss_prime = loss_prime\n\n    # predict output for given input\n    def predict(self, input_data):\n        \n        set = len(input_data)\n        result = []\n\n        #run \n        for i in range(set):\n            # forward propagation\n            output = input_data[i]\n            for eachlayer in self.layers:\n                output = eachlayer.forward_prop(output)\n            result.append(output)\n\n        return result\n\n    # train \n    def fit(self, x_train, y_train, epochs, learningr):\n        \n        samples = len(x_train)\n\n        # training \n        for i in range(epochs):\n            err = 0\n            for j in range(samples):\n                # forward propagation\n                output = x_train[j]\n                for layer in self.layers:\n                    output = layer.forward_prop(output)\n\n                # loss \n                err += self.loss(y_train[j], output)\n\n                # backward propagation\n                error = self.loss_prime(y_train[j], output)\n                for layer in (self.layers)[::-1]:\n                    error = layer.backward_prop(error, learningr)\n\n            # calculate average error \n            err /= samples\n            print('epoch %d/%d   error=%f' % (i+1, epochs, err))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Network class to build networks using previous classes and methods within, length of training and learning rate are decided in the fit method, where the network iterates over the layers, adjusting weights/biases through the object methods.Prints epoch and average error to the terminal.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Testing:\nDuring testing the following dataset will be used\n(0,0),(0,1),(1,0),(1,1)\n  0  ,  1  ,  1  ,  0\n This is because the operation used is simple, allowing for logical pathways to simply and quickly be tested for errors.\n ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "TEST NETWORK:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nfrom network import Network\nfrom fc_layer import FCLayer\nfrom activation_layer import ActivationLayer\nfrom housekeeping import tanh, tanh_prime\nfrom losses import mse, mse_prime\n\n# training data\nx_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\ny_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n\n# network\nnet = Network()\nnet.add(FCLayer(2, 3))\nnet.add(ActivationLayer(tanh, tanh_prime))\nnet.add(FCLayer(3, 1))\nnet.add(ActivationLayer(tanh, tanh_prime))\n\n# train\nnet.use(mse, mse_prime)\nnet.fit(x_train, y_train, epochs=1000, learningr=0.1)\n\n# test\nout = net.predict(x_train)\nprint(out)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Initial code outputs:\n    [array([[0.52692813]]), array([[0.52822712]]), array([[0.5136926]]), array([[0.51501981]])]\nThis indicated an error somewhere in the code, as all results would be returning 1(1sf), which indicated an error in the coding of the loss function.\nThis was resolved by changing an operation from ** to np.power().\n\nResults after bug fix:\n    [array([[0.0007457]]), array([[0.97984435]]), array([[0.97532576]]), array([[-0.0014717]])]\nwhich gives an extremely close output to the expectations of the XOR gate.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Conclusion:\nFrom the results of the code, it is suitable for training on simple problems in a short time, however as in the analysis it is unsuitable for some tasks such as image processing as only simple layers have been implemented. Additionally with more complex problems due to the lack of concurrency in program training which would make it take too long and not able to fully utilise any processor or GPU avaliable to it. ",
      "metadata": {}
    }
  ]
}